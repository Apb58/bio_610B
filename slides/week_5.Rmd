---
title: "Metric data: robustness and differences of 'means'"
author: "Peter Ralph"
date: "5 February 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(lars)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

# Overview

## Summary

So far we've focused on *discrete* data:
coin flips and counts.

While doing that we came across familiar things,
like Poisson regression.

This week we'll look at *continuous* (i.e., "metric") data.
Different sorts of *predictors* will lead us to different classical statistics:
linear regression, $t$-tests, etcetera.

But, control of the underlying model
will let us easily get more sophisticated,
including for instance
robustness to error, and penalization for overdetermined problems.


# The basic ingredients

*Fact:* standard linear regression
is a maximum likelihood estimate 
for $b_0$ and $b_1$ under the following model:
$$\begin{aligned}
    Y_i &= b_0 + b_1 X_1 + \epsilon_i \\
    \epsilon_i &\sim \Normal(0, \sigma) .
\end{aligned}$$


## Comparison of means

If the predictor, $X$, is *discrete*
then we are doing a $t$-test, or ANOVA, or something.

. . .

Simulate data - difference in means of 3.0:

```{r simdata_t}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- sample(c(0, +1), size=n, replace=TRUE)
y <- truth$b0 + truth$b1 * x + rnorm(n, mean=0, sd=truth$sigma)
```

------------------

The $t$-test
```{r tt}
system.time( tt <- t.test(y ~ x) )
tt
```

-----------------------------

with Stan
```{r stantt, cache=TRUE}
stt_block <- "
data { 
    int N;
    vector[N] x; // will be a vector of 0's and 1's
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(b0 + b1*x, sigma);
}"
system.time( stantt <- stan(model_code=stt_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

--------------------------

```{r summary_stantt}
rstan::summary(stantt)
```


## Standard linear regression

:::::::::::::: {.columns}
::: {.column width="50%"}


Simulate data

```r
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- ( truth$b0 + truth$b1 * x 
        + rnorm(n, mean=0, sd=truth$sigma) )
```

:::
::: {.column width="50%"}


```{r simdata, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- truth$b0 + truth$b1 * x + rnorm(n, mean=0, sd=truth$sigma)
plot(x,y)
abline(truth$b0, truth$b1, col='red')
```

:::
::::::::::::::



------------------

Standard linear regression
```{r slr}
system.time( slr <- lm(y ~ x) )
summary(slr)
```

-----------------

with Stan
```{r stanlr, cache=TRUE}
slr_block <- "
data {
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(b0 + b1*x, sigma);
}"
system.time( stanlr <- stan(model_code=slr_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

----------------


```{r summary_stanlr}
rstan::summary(stanlr)
```


## Multivariate linear regrssion

Simulate data:

```{r simdata_mv}
truth <- list(b0=1.0, 
              b=c(3.0, -1.0, 0.0, 0.0),
              sigma=0.5)
n <- 200
x <- matrix(rnorm(4*n, mean=0, sd=3), ncol=4)
y <- truth$b0 + x %*% truth$b + rnorm(n, mean=0, sd=truth$sigma)
```

. . .

**Your turn:** compare standard multivariate regression
to Stan.

*Note:* `%*%` in R, and `*` in Stan, are *matrix multiplication*.

------------------

Standard linear regression
```{r mlr}
system.time( mlr <- lm(y ~ x) )
summary(mlr)
```

-----------------

with Stan
```{r stanmlr, cache=TRUE}

mlr_block <- "
data {
    int N;
    matrix[N,4] x;
    vector[N] y;
}
parameters {
    real b0;
    vector[4] b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(to_vector(b0 + x * b1), sigma);
    b0 ~ normal(0, 10);
    b1 ~ normal(0, 10);
    sigma ~ normal(0, 10);
}"
system.time( stanmlr <- stan(model_code=mlr_block,
                            data=list(N=nrow(x), x=x, 
                                      y=as.vector(y)), iter=1e3) )

```

----------------


```{r summary_stanmlr}
rstan::summary(stanmlr)
```



# Testing with crossvalidation

## Crossvalidation

Just because a model *fits* doesn't mean that it's any *good*.

. . .

1. Divide your data randomly into 5 pieces.

2. Fit your model on 4/5ths, and see how well it predicts the remaining 1/5th.

3. Do this for each of the 5 pieces.

A better model should have better *crossvalidation accuracy*.

. . .

**Question:** for linear regression,
how do we "see how well it predicts"?
Write down the math, then code it up.


-----------------

```{r do_crossval}
five_groups <- sample(rep(1:5, each=nrow(x)/5))

do_crossval <- function (k) {
    # fit using not group k
    subset_lm <- lm( y ~ x, subset=(five_groups != k))
    # predict on group k
    yhat <- predict(subset_lm, 
                    newdata=list(x=x[five_groups == k,]))
    # compute crossvalidation accuracy
    S <- sqrt( mean( ( y[five_groups == k] - yhat  )^2 ) )
    return(S)
}

crossvals <- sapply(1:5, do_crossval)
```
The mean crossvalidation score for ordinary linear regression
was `r mean(crossvals)`,
with a standard deviation of `r sd(crossvals)`.



## What is overfitting?

Even completely independent sets of numbers
will correlate a little, because of noise.

. . .

When you have a *lot* of variables,
there may be some that correlate well with the response variable just by chance.


. . .

If you have as many variables than observations,
then there is (almost) *always* a linear model that fits *perfectly* (with $\epsilon = 0$).


----------------

*Example:*

```{r overparameterized}
nvars <- 200
truth <- list(b0=1.0, 
              b=rep(0.0, nvars),
              sigma=0.5)
n <- 200
x <- matrix(rnorm(nvars*n, mean=0, sd=3), ncol=nvars)
y <- truth$b0 + x %*% truth$b + rnorm(n, mean=0, sd=truth$sigma)
the_lm <- lm(y ~ x)
range(predict(the_lm) - y)
```



# Stochastic minute

---------------

If $X \sim \Cauchy(\text{center}=\mu, \text{scale}=\sigma)$, then $X$ has probability density
$$\begin{aligned}
    f(x \given \mu, \sigma) = \frac{1}{\pi\left( 1 + \left( \frac{x - \mu}{\sigma} \right)^2 \right)} .
\end{aligned}$$

> 1. The Cauchy is a good example of a distribution with "heavy tails":
>    rare, very large values.
> 
> 2. If $Y$ and $Z$ are independent $\Normal(0,1)$ then $Y/Z \sim \Cauchy(0,1)$.
>
> 3. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
>    $\max(X_1, \ldots, X_n)$ is of size $n$.
> 
> 4. Also, $\frac{1}{n}(X_1 + \ldots + X_n) \sim \Cauchy(0,1)$.

----------------

4. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
   $$\begin{aligned}
    \frac{1}{n} \left(X_1 + \cdots + X_n\right) \sim \Cauchy(0,1) .
   \end{aligned}$$

*Wait, what?!?*

. . .

A single value has the *same distribution* as the mean of 1,000 of them?

. . .

Let's look:
```{r cauchy_mean}
meanplot <- function (rf, n=1e3, m=100) {
    x <- matrix(rf(n*m), ncol=m)
    layout(t(1:2))
    hist(x[1,][abs(x[1,])<5], breaks=20, freq=FALSE,
         main=sprintf("%d samples", m),
         xlim=c(-5,5))
    hist(colMeans(x)[abs(colMeans(x))<5], breaks=20, freq=FALSE,
         main=sprintf("%d means of %d each", m, n),
         xlim=c(-5,5))
}
```

----------

$X \sim \Normal(0,1)$
```{r normmeans}
meanplot(rnorm)
```

-----------

$X \sim \Cauchy(0,1)$
```{r cauchymeans}
meanplot(rcauchy)
```

## Another way to look at it: extreme values

```{r max_values}
n <- 100
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values2}
n <- 1000
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values3}
n <- 1e6
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```



# Problem #1: noise

## Cauchy noise

Let's see what happens if 
$$\begin{aligned}
    Y_i &= b_0 + b_1 X_1 + \epsilon_i \\
    \epsilon_i &\sim \Cauchy(0, \sigma) .
\end{aligned}$$

. . .

```{r simdata_c}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 1000
x <- sample(c(0, +1), size=n, replace=TRUE)
y <- truth$b0 + truth$b1 * x + rcauchy(n, location=0, scale=truth$sigma)
```

------------------

The $t$-test
```{r ct}
system.time( tt <- t.test(y ~ x) )
tt
```

-----------------------------

with Stan
```{r stantct, cache=TRUE}
sct_block <- "
data { 
    int N;
    vector[N] x; // will be a vector of 0's and 1's
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model { 
    y ~ cauchy(b0 + b1*x, sigma);
}"
system.time( stanct <- stan(model_code=sct_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

--------------------------

```{r summary_stanct}
rstan::summary(stanct)
```

-----------

Because we *correctly model* the noise,
using Stan,
we are not thrown off by large values.



# Robust ANOVA

## Metric data from groups

Suppose we have numerical observations coming from $m$ different groups,
and want to know if the *means* are different between groups, and by how much.

. . .

:::::::::::::: {.columns}
::: {.column width="50%"}

e.g., dry leaf mass after 10d of growth
in 6 different conditions


:::
::: {.column width="50%"}


```{r simdata_ra, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim}
m <- 20
truth <- list(b=200 + rnorm(m, sd=5), 
              sigma=2.0)
n <- 200
x <- sample(1:m, size=n, replace=TRUE)
y <- pmax(0, truth$b[x]
              + rcauchy(n, location=0, 
                      scale=truth$sigma) )
```


:::
::::::::::::::


-----------------


:::::::::::::: {.columns}
::: {.column width="50%"}

dry leaf mass after 10d of growth
in 6 different conditions

```r
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```


:::
::: {.column width="50%"}

```{r plotdata_ra, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::


-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

Each group has a different location and scale;
noise about these is Cauchy;
group locations are random deviations;
group scale parameters are random deviations from a common scale.


$$\begin{aligned}
    Y_i &\sim \Cauchy(b_{g_i}, \sigma_{g_i}) \\
    \sigma_g &\sim \log\Normal(\mu, 1) \\
    b_g &\sim \Normal(0, \eta) \\
    \eta &\sim \Normal(0, 15) \\
    \mu &\sim \Normal(0, 1)
\end{aligned}$$

:::
::: {.column width="50%"}

```{r plotdata_ra4, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::


-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Y_i &\sim \Cauchy(b_{g_i}, \sigma_{g_i}) \\
    \sigma_g &\sim \log\Normal(\mu, 1) \\
    b_g &\sim \Normal(z, \eta) \\
    z &\sim \Normal(0, 10) \\
    \eta &\sim \Normal(0, 15) \\
    \mu &\sim \Exp(1)
\end{aligned}$$

*Changes:*

1. Added common mean, $z$.

2. Put "shrinkage" prior on $\mu$.

:::
::: {.column width="50%"}

```{r plotdata_ra3, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::

-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}


```{r do_anova, cache=TRUE}
anova_block <- "
data {
    int N; // number of obs
    int m; // number of groups
    vector[N] y; // leaf mass
    int g[N]; // group ID
}
parameters {
    vector[m] b;  // group 'means'
    real z;  // overall mean
    // group 'SD's
    vector<lower=0>[m] sigma; 
    // variability in group means
    real<lower=0> eta;  
    // log-mean of sigma
    real mu; 
}
model {
    y ~ cauchy(b[g], sigma[g]);
    // lognormal, encouraging all group sigmas to be the same,
    // but trying not to constrain them if they want to be different
    sigma ~ lognormal(mu, 1);
    // unsure of scale for b, 
    // so put a hyperprior on it
    b ~ normal(z, eta);
    z ~ normal(0, 15);
    eta ~ normal(0, 15);
    // encourage mu to be near zero if reasonable
    mu ~ exponential(1);
}"

## note we subtract the median of y
anova_data <- list(N=length(y),
                   m=length(unique(x)),
                   y=y - median(y),
                   g=x)

anova_fit <- stan(model_code=anova_block,
                  data=anova_data,
                  iter=1e3,
                  control=list(adapt_delta=0.99))
```

:::
::: {.column width="50%"}

```{r plotdata_ra2, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::

--------------

```{r anova_run_summary}
rstan::summary(anova_fit)
```

--------------

:::::::::::::: {.columns}
::: {.column width="50%"}


*What do we want to know?*

1. Do the treatments affect mean growth?

2. How much does leaf mass in treatment 8 differ from group 12?

3. How much variation is explained by treatment?

:::
::: {.column width="50%"}


*How do we find it out?*

1. See if any credible intervals overlap?
   And, see if credible interval for eta > 0.

2. Posterior distribution of `b[12] - b[8]`: if `>0` then `b[12] > b[8]`; if zero is in it then `b[12] = b[8]`.

3. Compare `sigma` to `eta`.
   Or take the ratio of MADs before and after subtracting `b[-]`.

:::
::::::::::::::


## Do treatments affect growth?

First let's simply see if the credible intervals overlap.
Many of these don't overlap even close,
so it sure looks like some of the treatments were different
from other ones.

```{r overlapping_intervals}
quietgg(stan_plot(anova_fit, pars="b"))
```

---------------

Now, let's look at `eta`, which is the SD parameter
on the prior for `b`, the group location parameters.
If `eta` is zero, then all the `b`s are the same.

It looks like `eta` has a wide posterior distribution,
but all the posterior support is quite far from zero,
indicating that the group locations are different between the groups.

```{r look_at_eta}
rstan::summary(anova_fit, pars="eta")$summary
```

------------

```{r eta_hist}
stan_hist(anova_fit, pars="eta", bins=30)
```



## How much do treatments 8 and 12 differ?

Here we look at the posterior distribution of `b[12] - b[8]`.

```{r post_b_diff}
bsamps <- extract(anova_fit)$b
diff_samps <- bsamps[,12] - bsamps[,8]
hist(diff_samps, main="posterior of b[12] - b[8]", xlab='b[12] - b[8]')
```

-----------

The posterior mean difference is `r mean(diff_samps)`
but a 95% credible interval goes from `r quantile(diff_samps, prob=.025)` to 
`r quantile(diff_samps, prob=.975)`.
Since zero is in the credible interval,
we can't really tell from the data if there is a real difference here,
but if there is, it's probably less than 5 or so.



## How much variation is explained by treatment?

We want to compare `eta` (which describes how much
different treatments affect leaf mass)
to the values of `sigma` (which describe how much leaf mass varies
within each treatment).

First we need to check how much `sigma` vary.

```{r look_at_sigma}
quietgg(stan_plot(anova_fit, pars="sigma"))
```

-------------

The posterior distributions of different groups' sigma values
overlap a lot; so we'll look at the posterior distribution
of the mean sigma value to describe how much within-group variation there is.

```{r mean_sigma}
mean_sigmas <- rowMeans(extract(anova_fit, pars="sigma")$sigma)
hist(mean_sigmas)
```

It looks like `sigma` is between 2 and 4, roughly.

-----------

We really want to know how big `eta` tends to be relative to `sigma`

```{r ratio_distrn}
eta_samples <- extract(anova_fit)$eta
ratio_samples <- eta_samples / mean_sigmas
hist(ratio_samples)
```

-----------

The posterior mean ratio of `eta` to `sigma` is `r mean(ratio_samples)`,
with a 95% credible interval of
`r quantile(ratio_samples, probs=.025)`
to
`r quantile(ratio_samples, probs=.975)`.
In other words, it looks like
differences in leaf mass between treatments
are almost twice the typical differences between plants
that had the same treatment.

