---
title: "Metric data: regression and relatives"
author: "Peter Ralph"
date: "12 February 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(lars)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

```{r helpers, include=FALSE}
shadecurve <- function (pf, xlim, plot=TRUE, xlab='', ylab='', main='',
                        border="black", col=adjustcolor(border, 0.25), ...) {
    x <- seq(xlim[1], xlim[2], length.out=400)
    mids <- x[-1] - diff(x)/2
    df <- diff(pf(x, ...))
    if (plot) { plot(0, type='n', xlim=range(x), ylim=range(df),
                     main=main, xlab=xlab, ylab=ylab) }
    polygon(c(mids, x[length(x)], x[1]), c(df, 0, 0), col=col, border=border) 
}
```

# Overview

## Summary

Last Thursday, we looked at:

1. Using non-Normal noise distributions (e.g., Cauchy) to make models *robust* to outliers
   and more generally model overdispersed data.

2. Interrogating hyperparameters to learn what we want (e.g., percent variation explained).

## Today

0. Finish up robust ANOVA.

1. Problem #1: "too much" noise (i.e., non-Normal noise).

2. Problem #2: too many variables.

# Robust regression

## Some data

:::::::::::::: {.columns}
::: {.column width="50%"}


Relative axon growth
for neurons after $x$ hours:

```r
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- ( truth$b0 + truth$b1 * x 
        + rcauchy(n, location=0, 
                  scale=truth$sigma) )
```

:::
::: {.column width="50%"}

```{r simdata_rr, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- truth$b0 + truth$b1 * x + rcauchy(n, location=0, scale=truth$sigma)
plot(x,y)
abline(truth$b0, truth$b1, col='red')
```

:::
::::::::::::::


------------------

Standard linear regression
```{r slrr}
system.time( slr2 <- lm(y ~ x) )
summary(slr2)
```

-----------------

with Stan
```{r stanrr, cache=TRUE}
srr_block <- "
data { 
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ cauchy(b0 + b1*x, sigma);
}"
system.time( stanrr <- stan(model_code=srr_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

----------------


```{r summary_stanrr}
rstan::summary(stanrr)
```

## Compare the results.

Make a table and/or a graph of estimates and confidence intervals
obtained by (a) ordinary linear regression
and (b) robust regression as we have done here.




# Problem #2: too many variables


## Example data

![from Efron, Hastie, Johnstone, \& Tibshirani](lars_diabetes_data.png)

-----------

```{r lars_data}
library(lars)
data(diabetes)
class(diabetes$x2) <- "matrix"
```
```
diabetes                 package:lars                  R Documentation

Blood and other measurements in diabetics

Description:

     The ‘diabetes’ data frame has 442 rows and 3 columns. These are
     the data used in the Efron et al "Least Angle Regression" paper.

Format:

     This data frame contains the following columns:

     x a matrix with 10 columns

     y a numeric vector

     x2 a matrix with 64 columns
```

---------------

The dataset has

- 442 diabetes patients
- 10 main variables: age, gender, body mass index, average blood pressure (map), 
  and six blood serum measurements (tc, ldl, hdl, tch, ltg, glu)
- 45 interactions, e.g. `age:ldl`
- 9 quadratic effects, e.g. `age^2`
- measure of disease progression taken one year later, `y`

## Crossvalidation plan

1. Put aside 20% of the data for *testing*.

2. Refit the model.

3. Predict the test data; compute
   $$\begin{aligned}
    S = \sqrt{\frac{1}{M} \sum_{k=1}^M (\hat y_i - y_i)^2}
   \end{aligned}$$

4. Repeat for the other four 20%s.

5. Compare.


## Crossvalidation

First let's split the data into testing and training just once:
```{r split_data}
test_indices <- (rbinom(length(diabetes$y), size=1, prob=0.2) == 1)
test_d <- cbind(data.frame(y=diabetes$y[test_indices]),
                diabetes$x2[test_indices,])
training_d <- cbind(data.frame(y=diabetes$y[!test_indices]),
                diabetes$x2[!test_indices,])
```

## Ordinary linear regression

```{r ols}
ols <- lm(y ~ ., data=training_d)
summary(ols)
```

-------------

```{r crossval_ols}
ols_pred <- predict(ols, newdata=test_d)
ols_mse <- sqrt(mean((ols_pred - test_d$y)^2))
```
With ordinary linear regression, we got a root-mean-square-prediction-error of `r ols_mse` (on the *test* data),
compared to a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` for the *training* data.

. . .

This suggests there's some overfitting going on.

------------

```{r plot_ols}
layout(t(1:2))
plot(training_d$y, predict(ols), xlab="true values", ylab="OLS predicted", main="training data")
abline(0,1)
plot(test_d$y, ols_pred, xlab="true values", ylab="OLS predicted", main="test data")
abline(0,1)
```

## A sparsifying prior

We have a lot of predictors: 64 of them.
A good guess is that only a few are really useful.
So, we can put a *sparsifying* prior on the coefficients,
i.e., the $\beta$s in
$$\begin{aligned}
    y = \beta_0 + \beta_1 x_1 + \cdots \beta_n x_n + \epsilon
\end{aligned}$$


# Sparseness and scale mixtures

## Encouraging sparseness

Suppose we do regression with a *large* number of predictor variables.

. . .

The resulting coefficients are **sparse** if most are zero.

. . .

The idea is to "encourage" all the coefficients to be zero,
**unless**  they *really* want to be nonzero,
in which case we let them be whatever they want.

. . .

This tends to discourage overfitting.

-------------

> The idea is to "encourage" all the coefficients to be zero,
> **unless**  they *really* want to be nonzero,
> in which case we let them be whatever they want.


To do this, we want a prior which is very peak-ey at zero
*but* flat away from zero ("spike-and-slab").


-------------------

:::::::::::::: {.columns}
::: {.column width="50%"}


Compare the Normal

$$\begin{aligned}
    X \sim \Normal(0,1)
\end{aligned}$$

to the "exponential scale mixture of Normals",

$$\begin{aligned}
    X &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```{r scale_mixtures, fig.width=1.5*fig.dim, echo=FALSE}
shadecurve(pnorm, xlim=c(-5,5), main='Normal(0,1)')
sdvals <- qexp(seq(0,1,length.out=10)[-c(1,10)])
for (u in sdvals) {
    shadecurve(pnorm, xlim=c(-5,5), sd=u, plot=(u==sdvals[1]), main='Normal(0, Exp(1))')
}
```

:::
::::::::::::::

## Why use a scale mixture?

1. Lets the data choose the appropriate scale of variation.

2. Weakly encourages $\sigma$ to be small: so, 
   as much variation as possible is explained by *signal* instead of *noise*.

3. Gets you a prior that is more peaked at zero and flatter otherwise.


## Implementation

:::::::::::::: {.columns}
::: {.column width="50%"}


Note that

$$\begin{aligned}
    \beta &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

is equivalent to

$$\begin{aligned}
    \beta &= \sigma \gamma \\
    \gamma &\sim \Normal(0,1) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
}
```

is equivalent to

```
parameters {
    real gamma;
    real<lower=0> sigma;
}
transformed parameters {
    real beta;
    beta = gamma * sigma;
}
model {
    beta ~ normal(0, sigma);
}
```

The second version **is better** for Stan.


:::
:::::::::::::::

-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

Why is it better?


```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
}
```

In the first, the optimal step size
*depends on `sigma`*.

:::
::: {.column width="50%"}

```{r sigma_phase, echo=FALSE, fig.width=1.5*fig.dim}
xx <- seq(0, 0.5, length.out=401)[-1]
yy <- seq(-4,4,length.out=401)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=x) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="beta")
contour(xx, yy, -log10(xy), add=TRUE)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=1) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="gamma")
contour(xx, yy, -log10(xy), add=TRUE)
```

:::
:::::::::::

## A strongly sparsifying prior



:::::::::::::: {.columns}
::: {.column width="50%"}

The "horseshoe":

$$\begin{aligned}
    \beta_j &\sim \Normal(0, \lambda_j) \\
    \lambda_j &\sim \Cauchy(0, \tau) \\
    \tau &\sim \Unif(0, 1) 
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    vector[p] d_beta;
    vector[p] d_lambda;
    real<lower=0, upper=1> tau;
}
transformed parameters {
    vector[p] beta;
    beta = d_beta .* d_lambda * tau;
}
model {
    d_beta ~ normal(0, 1);
    d_lambda ~ cauchy(0, 1);
    // tau ~ uniform(0, 1); // uniform
}
```

:::
:::::::::::

# The Cauchy as a scale mixture

## Inverse square-root gamma scaling

It turns out that if

$$\begin{aligned}
    \beta &\sim \Normal(0, 1/\sqrt{\lambda}) \\
    \lambda &\sim \Gam(1/2, 1/2)
\end{aligned}$$

then

$$\begin{aligned}
    \beta &\sim \Cauchy(0, 1).
\end{aligned}$$

. . .

*What black magic is this??*

. . .

1. It says so [here](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html).

2. If you like to do integrals, you can check mathematically.

3. Or, you can check with *simulation*.



# Using the horseshoe

## What's an appropriate noise distribution?

```{r show_y, echo=FALSE, fig.width=3*fig.dim, fig.height=1.5*fig.dim}
layout(t(1:2))
hist(diabetes$y, breaks=30, main="diabetes, response")
qqnorm(diabetes$y)
qqline(diabetes$y)
```

## Aside: quantile-quantile plots

The idea is to plot the *quantiles* of each distribution against each other.

If these are *datasets*, this means just plotting their *sorted values* against each other.

```{r qq, fig.width=3*fig.dim}
x <- rnorm(1e4)
y <- rbeta(1e4, 2, 2)

layout(t(1:3))
plot(sort(x), sort(y))
qqplot(x, y, main="qqplot")
qqnorm(y, main="qnorm")
```


## Regression with a horseshoe prior

Uses a [reparameterization](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html) of the Cauchy as a scale mixture of normals.


```{r horseshoe_model, cache=TRUE}
horseshoe_block <- "
data {
    int N;
    int p;
    vector[N] y;
    matrix[N,p] x;
}
parameters {
    real b0;
    vector[p] d_beta;
    vector[p] d_a;
    vector<lower=0>[p] d_b;
    real<lower=0, upper=1> tau;
    real<lower=0> sigma;
}
transformed parameters {
    vector[p] beta;
    vector[N] f;
    beta = d_beta .* d_a .* sqrt(d_b) * tau;
    f = b0 + x * beta;
}
model {
    y ~ normal(f, sigma);
    // HORSESHOE PRIOR:
    d_beta ~ normal(0, 1);
    d_a ~ normal(0, 1);
    d_b ~ inv_gamma(0.5, 0.5);
    // tau ~ uniform(0, 1); // uniform
    // priors on noise distribution:
    sigma ~ normal(0, 10);
}"
```

------------------

Note the data have already been normalized,
with the exception of $y$:

```{r data_summary}
summary(training_d)
```

------------------

```{r run_horseshoe, cache=TRUE, depends="horseshoe_model"}
horseshoe_fit <- stan(model_code=horseshoe_block,
                      data=list(N=nrow(training_d),
                                p=ncol(training_d)-1,
                                y=(training_d$y 
                                   - median(training_d$y))
                                  /mad(training_d$y),
                                x=as.matrix(training_d[,-1])),
                      iter=1000,
                      control=list(adapt_delta=0.999,
                                   max_treedepth=15))
```

--------------

```{r summary_hs}
(hs_summary <- rstan::summary(horseshoe_fit, pars=c("b0", "sigma", "beta")))
```

--------------

First compare the resulting regression parameters to OLS values.

```{r compare_betas, echo=1:5, fig.width=3*fig.dim}
hs_samples <- extract(horseshoe_fit, pars=c("b0", "sigma", "beta"))
# rescale to real units
post_med_intercept <- median(hs_samples$b0) * mad(training_d$y) + median(training_d$y)
post_med_sigma <- median(hs_samples$sigma) * mad(training_d$y)
post_med_slopes <- colMedians(hs_samples$beta) * mad(training_d$y)

ols_ses <- summary(ols)$coefficients[,2]

layout(t(1:2))
plot(coef(ols), xlab="coefficient", 
     ylab="estimate", main="OLS", pch=20)
segments(x0=1:65,
         y0=coef(ols) - 2 * ols_ses,
         y1=coef(ols) + 2 * ols_ses,
         col=adjustcolor("black", 0.5))
plot(c(post_med_intercept, post_med_slopes), 
     xlab='coefficient', ylab="estimate",
     col='red', pch=20, main="sparse")
segments(x0=1, 
         y0=mad(training_d$y) * quantile(hs_samples$b0, probs=0.025) + median(training_d$y), 
         y1=mad(training_d$y) * quantile(hs_samples$b0, probs=0.975) + median(training_d$y), 
         col=adjustcolor('red', 0.5))
segments(x0=2:65, 
         y0=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.025), 
         y1=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.975),
         col=adjustcolor('red', 0.5))
```


--------------

The coefficient estimates from OLS are *wierd*.

```{r what_coefs, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$ols[-1]), decreasing=TRUE)),]
```

--------------

And, quite different than what Stan gets.

```{r what_coefs2, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$stan[-1]), decreasing=TRUE)),]
```



--------------

Now let's look at out-of-sample prediction error,
using the posterior median coefficient estimates:

```{r pred_stan}
pred_stan <- function (x) {
    post_med_intercept + as.matrix(x) %*% post_med_slopes
}
pred_y <- pred_stan(test_d[,-1])
stan_pred_error <- sqrt(mean((test_d$y - pred_y)^2))
stan_mse_resid <- sqrt(mean((training_d$y - pred_stan(training_d[,-1]))^2))

plot(test_d$y, pred_y, xlab="true values", ylab="predicted values", main="test data")
abline(0,1)
```

## Conclusions?


1. Our "sparse" model is certainly more sparse, and arguably more interpretable.

2. It has a root-mean-square prediction error of 
    `r stan_pred_error`
    on the *test* data, and
    `r stan_mse_resid`
    on the training data.

3. This is substantially better than ordinary linear regression, 
   which had a root-mean-square prediction error of `r ols_mse` on the test data,
   and a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` on the training data.


. . .

The sparse model is more interpretable,
and more generalizable.

